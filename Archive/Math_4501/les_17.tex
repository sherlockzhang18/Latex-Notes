\newpage
\lesson{17}{Mon Oct 13 2025 15:00}{}

Review: \underbar{\textbf{Find $ \xi $ such that $ f(\xi) = 0 $}}

\begin{itemize}
    \item 1d: $ g(\xi) = \xi $ fixed point problem
    
    To solve $ f(x) = 0 $
    \begin{itemize}
        \item $ g(x) = x - f(x) $ Brittle
        \item $ g_{\lambda}(x) = x-\lambda f(x) $ (better)
        \item $ x_{k+1} = x_{k} - \frac{f(x_{k})}{x_{k}} $ (Newton)
    \end{itemize}
\end{itemize}


\vspace{2em}
\begin{eg} \leavevmode
    
$ f:\mathbb{R}^2 \to \mathbb{R}^2 $

$ 
    f(x) = f(x_1, x_2) = 
    \begin{bmatrix}
    f_1(x_1,x_2) \\
    f_2(x_1,x_2) \\
    \end{bmatrix}
    = \begin{bmatrix}
    x_1^2 + x_2^2-1 \\
    5x_1^2+21x_2^2-9 \\
    \end{bmatrix}
$

To solve, let $ f(x) = 0 $
\[
    \begin{cases}
        &x_1^2 + x_2^2 = 1 \\
        &5x_1^2 + 21x^2 = 9 \\
    \end{cases}
\]

Like above, we need to form $ g(x) = x - f(x) $.

Want $ g $ to be a contraction at fixed points.

\vspace{1em}
Use jacobian as analog for derivative, and use infinite norm for simplicity.

so check $ \left\lVert J_{g}(\xi) \right\rVert _\infty < 1 $.

\[
    g(x) = 
    \begin{bmatrix}
        x_1-x_1^2-x_2^2 + 1 \\
        x_2 - 5x_1^2 - 21x^2 + 9\\
    \end{bmatrix}
\]

\vspace{1em}
\[
    J_{g}(x) = 
    \begin{bmatrix}
      1 - 2x_1 & -2x_2 \\
      -10x_1 & 1 - 42x^2 \\
    \end{bmatrix}
\]

so 
\[
    \left\lVert J_{g}(x) \right\rVert _ \infty = \max\{|1-2x_1| + |2x_2|, |10x_1| + |1 - 42x_2|\}
\]

\[
    \left\lVert J_{g}(x) \right\rVert _\infty < 1 
    \implies
    \begin{cases}
        |x_1 - \frac{1}{2}| + |x_2| < \frac{1}{2} \\
        10|x_1| + 42|x-\frac{1}{42}| < 1
    \end{cases}
\]

This is bad since the range doesn't include the fixed points by graph.

\vspace{2em}
Try $ g_{\lambda}(x) = x - \lambda f(x) $. Here use $ \lambda = \frac{1}{50} $

then,
\[
    J_{g}(x) = 
    \begin{bmatrix}
      1 - \frac{2}{50}x_1 & -\frac{2}{50}x_2 \\
      -\frac{10}{50}x_1 & 1 - \frac{42}{50}x^2 \\
    \end{bmatrix}
\]

\[
    \left\lVert J_{g}(x) \right\rVert _ \infty = \max\{|1-\frac{2}{50}x_1| + |\frac{2}{50}x_2|, |\frac{10}{50}x_1| + |1 - \frac{42}{50}x_2|\}
\]

\[
    \left\lVert J_{g}(x) \right\rVert _\infty < 1 
    \implies
    \begin{cases}
        |x_1 - \frac{1}{2}| + |x_2| < \frac{1}{2} \\
        10|x_1| + 42|x-\frac{1}{42}| < 1
    \end{cases}
\]
changes needed here above in the lines.

Then the fixed points are included then.

\end{eg}

\vspace{2em}
\begin{definition} Relaxation Iteration \leavevmode

    \[
        g_{\lambda}(x) = x - \lambda f(x)
    \]
    \[
        x_{k+1} = g_{\lambda}(x_{k})
    \]
    
\end{definition}

\vspace{2em}
\begin{theorem}
    $ f: \mathbb{R}^n \to \mathbb{R}^n $, $ f(\xi) = 0 $
    \begin{itemize}
        \item $ \frac{\partial f_{i}}{\partial x_{j}} $ continuous in a neighbourhood of $ \xi $
        \item $ J_{f}(\xi) $ is "strictly diagonally dominant"
    \end{itemize}

    There exists $ \epsilon, \lambda > 0 $ such that Relax Iteration converges if $ x_0 \in B_{\epsilon}(\xi) $

\end{theorem}

\begin{note}
    $ J_{f}(\xi) $ is strictly diagonally if 
    \[
        \frac{\partial f_{i}}{\partial x_{j}} > \sum_{j = 1}^{n} \left| \frac{\partial f_{i}}{\partial x_{j}}(\xi) \right|
    \]

    \vspace{1em}
    \[
        J_{f}(\xi) = 
        \begin{bmatrix}
          0 & 0 & 0 \\
          \frac{df_1}{dx_1} & \cdots  & df_1 \\
          0 & 0 & 0 \\
        \end{bmatrix}    
    \]
\end{note}

\vspace{2em}
\begin{proof} Theorem 19 \leavevmode

    Need only show $ \left\lVert J_{g_{\lambda}}(\xi) \right\rVert _\infty < 1 $

    \[
        \left[ J_{g_{\lambda}} (x) \right] _{ij} 
        = 
        \begin{cases}
            1 - \lambda \frac{\partial f_{i}}{\partial x_{j}} \hspace{2em} i = j \\
            -\lambda \frac{df_1}{\partial x_{j}} \hspace{2em} i \neq j \\
        \end{cases}
    \]

    \begin{align*}
        \sum_{j = 1}^{n} \left| \frac{dgx_{i}}{\partial x_{j}} (\xi)\right| 
        &= \left|1-\lambda \frac{\partial f_{i}}{dx}(\xi)\right| + \sum_{j = 1}^{n} \lambda \left|\frac{\partial f_{i}}{\partial x_{j}} (\xi)\right| \\
        &= 1 - \lambda\left(\frac{\partial f_{i}}{\partial x_{i}}(\xi) - \sum_{j = 1, j \neq i}^{n} \left| \frac{\partial f_{i}}{\partial x_{i}} (\xi) \right|\right) \\
    \end{align*}

    For eliminating the absolute value in the above step, consider:

    Set $ \lambda = \frac{1}{\max_{i}\frac{\partial f_{i}}{\partial x_{i}}(\xi)} $

    so 
    \begin{align*}
        \left\lVert J_{g_{\lambda}}(\xi) \right\rVert _\infty
        &\leqslant \max_{i} 1 - \lambda\left(\frac{\partial f_{i}}{\partial x_{i}}(\xi) - \sum_{j = 1, j \neq i}^{n} \left| \frac{\partial f_{i}}{\partial x_{i}} (\xi) \right|\right) \\
        &\leqslant 1 - \lambda\frac{\partial f_{i}}{\partial x_{i}}(\xi) < 1
    \end{align*}

    Contraction mapping Theorem applies.
    
\end{proof}