\lesson{15}{Mon Sep 29 2025 15:00}{}

$ 
    \begin{cases}
        a_1 = \left\lVert q_1 \right\rVert _2 \cdot q_1 \\
        a_2 = \left\lVert \_ \right\rVert _2 \cdot (a_2 q_1) q_1
        a_3 = \sum_{i=1}^{j} a_{i} q_{i}
    \end{cases}
$

\[
    \mathbb{R}^{m \times k} \to \hat{Q}_{k} = \left[ q_1 | q_2 | \cdots | q_{k} \right] \hspace{2em} \hat{Q}_{k} ^T \hat{Q}_{k} = I_{k}
\]

$ Q^{k} \cdot Q_{k}^T $: projection matrix.

\vspace{1em}
$ \{q_1, \cdots , q_{k}\} $ orthonormal basis. $ q \in \mathbb{R}^m $.

\[
    \hat{Q}_{k} = \left[ q_1 | \cdots | q_{k} \right]
\]

$ \hat{Q_{k}}\hat{Q}_k^T $: Projection matrix given a $ v \in \mathbb{R}^m, \hat{Q}_k \cdot \hat{Q_{k}}^T v $ is the projection of $ \overrightarrow{v} $ onto span$ \{q_1, \cdots , q_{k}\} $.

