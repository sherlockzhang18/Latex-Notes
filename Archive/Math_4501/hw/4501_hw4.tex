\documentclass[a4paper]{article}

\usepackage[margin=1.3in]{geometry} % reduce margin

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{url}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[dvipsnames]{xcolor}
\usepackage{xifthen}

% Math packages
\usepackage{amsmath, amsfonts, mathtools, amsthm, amssymb}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{bm}
\usepackage{systeme}
\usepackage{stmaryrd} % for \lightning

% Math shortcuts
\newcommand\N{\ensuremath{\mathbb{N}}}
\newcommand\R{\ensuremath{\mathbb{R}}}
\newcommand\Z{\ensuremath{\mathbb{Z}}}
\renewcommand\O{\ensuremath{\emptyset}}
\newcommand\Q{\ensuremath{\mathbb{Q}}}
\newcommand\C{\ensuremath{\mathbb{C}}}
\newcommand\F{\ensuremath{\mathbb{F}}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Ker}{ker}
\DeclareMathOperator{\im}{Im}

% Logic symbols
\let\svlim\lim\def\lim{\svlim\limits}
\let\implies\Rightarrow
\let\impliedby\Leftarrow
\let\iff\Leftrightarrow
\let\epsilon\varepsilon
\newcommand\contra{\scalebox{1.1}{$\lightning$}}

% Useful commands
\definecolor{correct}{HTML}{009900}
\newcommand\correct[2]{\ensuremath{\:}{\color{red}{#1}}\ensuremath{\to }{\color{correct}{#2}}\ensuremath{\:}}
\newcommand\green[1]{{\color{correct}{#1}}}

% Horizontal rule
\newcommand\hr{
    \noindent\rule[0.5ex]{\linewidth}{0.5pt}
}

% Simple theorem environments (without fancy boxes for homework)
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem*{example}{Example}

% Problem environment
\newcounter{problem}
\newenvironment{problem}[1][]
{
    \stepcounter{problem}
    \section*{Problem \theproblem\ifx\relax#1\relax\else: #1\fi}
}
{}

% Subproblem environment
\newcounter{subproblem}[problem]
\newenvironment{subproblem}[1][]
{
    \stepcounter{subproblem}
    \subsection*{(\alph{subproblem})\ifx\relax#1\relax\else\ #1\fi}
}
{}

% Solution environment
\newenvironment{solution}
{
    \noindent\textbf{Solution:}\\
}
{
    
}

% Headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Sherlock Zhang}
\fancyhead[C]{4501 - Homework 4}
\fancyhead[R]{\today}
\fancyfoot[C]{\thepage}

% Title info
\title{Math 4501 - Homework 4}
\author{Sherlock Zhang}
\date{\today}

\begin{document}

\maketitle

% =============================================================================
% HOMEWORK PROBLEMS START HERE
% =============================================================================

\begin{problem}

    Consider the simultaneous equations $ f(x_1, x_2) = 0 $, where $ f = (f_1, f_2)^T $ with
    \[
        f_1(x_1, x_2) = x_1^2 + x_2^2 - 25, \hspace{1em} f_2(x_1, x_2) = x_1 - 7x_2 - 25,
    \]

\end{problem}

\begin{subproblem}

    Show taht the equations have two solutions, one of which is $ x_1 = 4, x_2 = -3 $. Find the other.

\end{subproblem}

\vspace{2em}
\begin{solution}

    From $ f_2 = 0 $ we know $ x_1 = 7x + 25 $. Substitute into $ f_1 $ we get
    \[
        (7x_2 + 25)^2 + x_2^2 = 25 \implies x_2^2 + 7x + 12 = 0 \implies x_2 = -3 \text{ or } -4
    \]
    then $ x_1 = 4 \text{ or } -3 $ correspondingly.

    Thus the two solutions are $ (4, -3) $ and $ (-3, -4) $.

\end{solution}

\begin{subproblem}

    Show that if $ f $ is replaced by $ f^{**} = (f_2 - f_1, -f_2)^T $, then the conditions are satisfied at the other solution.

\end{subproblem}
\vspace{1em}

\begin{solution}
    
    $ f_2 - f_1 = x_1 - 7x_2 - x_1^2 - x_2^2 $, $ f_1 = -x_1 + 7x_2 + 25 $.

    Need to check $ \frac{\partial f_{i}}{\partial x_{j}} $ is continuous in a neighborhood $ N(\xi) $ and $ J_{f^{**}}(\xi) $ is strictly diagonally dominant.

    \[
        J_{f^{**}}(x) 
        = 
        \begin{bmatrix}
          1-2x_1 & -7-2x_2 \\
          -1 & 7 \\
        \end{bmatrix}
    \]

    As $ \frac{\partial f_{i}}{\partial x_{j}} $ shown in  $ J_{f^{**}} $ are all polynomials and constants, they are all continuous around $ (-3, -4) $.

    \vspace{2em}
    \[
        J_{f^{**}}(\xi) 
        = 
        \begin{bmatrix}
          7 & 1 \\
          -1 & 7 \\
        \end{bmatrix}
    \]
    Since $ 7 > |1| $ and $ 7 > |-1| $, it is strictly diagonally dominant. Therefore the conditions are satisfied.

\end{solution}

\begin{subproblem}
    
    In both cases $ (\text{for } f^* \text{ and } f^{**}) $, give a value of the relaxation parameter $ \lambda $ which will lead to convergence.

\end{subproblem}

\begin{solution}
    
    For $ f^{**} $, since it satisfies the condition in theorem 4.3, use the method we proved in class:
    \[
        \lambda = \frac{1}{\max_{i = 1, \cdots , n} \left|\frac{\partial f_{i}}{\partial x_{i}}\right|}
    \]

    From (b) we know that $ \lambda = \frac{1}{7} $ can satisfy the convergence condition.

    For $ f $, 
    \[
        J_{f}(x) = \begin{bmatrix}
          2x_1 & 2x_2  \\
          1 & -7 \\
        \end{bmatrix}
    \]

    then 
    \[
        J_{g}(x) = 1 - J_{f}(\xi) = 1 - J_{f}(4, -3) =
        \begin{bmatrix}
          1-8\lambda & 6\lambda \\
          -\lambda & 1+7\lambda \\
        \end{bmatrix}
    \]

    So $ \left\lVert J_{g}(x) \right\rVert _\infty = 
    \max
    \begin{cases}
      |1-8\lambda| + |6\lambda| \\
      |-\lambda| + |1+7\lambda| \\
    \end{cases} $
    <1

    There is no real root for $ \lambda $ that can make $ \left\lVert J_{g}(x) \right\rVert _\infty < 1 $.

\end{solution}

\newpage
\setcounter{problem}{2}
\begin{problem}
    
    Suppose that all the second-order partial derivatives of the function $ f: \mathbb{R}^n \to \mathbb{R}^n $ are defined and continuous in a neighborhood of the point $ \xi $ in $ \mathbb{R}^n $, at which $ f(\xi) = 0 $. Assume also that the jacobian matrix $ J_{f}(x) $ of $ f $ is non-singular at $ x = \xi $, and denote its inverse by $ K(x) $ at all $ x $ for which it exists. Defining the Newton iteration by $ x^{(k+1)} = g(x^{(k)}), k = 0, 1, \cdots ,  $with $ x^{(0)} $ given, where $ g(x) = x - K(x)f(x) $, show that the $ (i, j) $-entry of the Jacobian matrix $ J_{g}(x) \in \mathbb{R}^{n \times n} $ its
    \[
        \delta_{ij} - \sum_{r=1}^{n} \frac{\partial K_{ir}}{\partial x_{j}}f_{r} - \sum_{r=1}^{n} K_{ir}J_{rj}, \hspace{1em} i,j = 1, \cdots , n,
    \]
    where $ J_{rj} $ is the $ (r,j) $-entry of $ J_{f}(x) $. Deduce that all the elements of this matrix vanish at point $ \xi $.

\end{problem}

\vspace{2em}
\begin{solution}
    
    By definition of Jacobian matrix,
    \[
        [J_{g}(x)]_ij 
        = \frac{\partial g_{i}}{\partial x_{j}} 
        = \frac{\partial}{\partial x_{j}}\left(x_{i} - \sum_{r=1}^{n} K_{ir}f_{r} \right) = \delta_{ij} - \sum_{r=1}^{n} \left(\frac{\partial K_{ir}}{\partial x_{j}} + K_{ir}\frac{\partial f_{r}}{x_{j}}\right)
    \]
    This is the same as the desired property:
    \[
        [J_{g}(x)]_ij = \delta_{ij} - \sum_{r=1}^{n} \frac{\partial K_{ir}}{\partial x_{j}}f_{r} - \sum_{r=1}^{n} K_{ir}J_{rj}, \hspace{1em} i,j = 1, \cdots , n,
    \]

    when $ f(\xi) = 0 $, the term $ \sum_{r=1}^{n} \frac{\partial K_{ir}}{\partial x_{j}}f_{r}(x) $ vanishes as it multiply by zero. 

    The other sum
    \[
        \sum_{r=1}^{n} K_{ir}(\xi)J_{rj}(\xi) = [K(\xi)J_{f}(\xi)]_ij = [J^{-1}_{f} J_{f}]_ij = \delta_{ij}
    \]
    Then 
    \[
        J_{g}(\xi) = \delta_{ij} - 0 - \delta_{ij} = 0
    \]

\end{solution}

\newpage
\begin{problem}
    
    Let $ f(x) $ be given by
    \[
        f_1 (x_1, x_2) =x_1^2 + x_2^2 - 2, \hspace{1em} f_2(x_1, x_2) = x_1 - x_2
    \]

\end{problem}

\begin{subproblem}
    
    Verify that the equation $ f(x) = 0 $ has two solutions, $ x_1 = x_2 = 1 $ and $ x_1 = x_2 = -1 $.

\end{subproblem}

\begin{solution}
    
    From $ f_2 = 0 $ we know $ x_1 = x_2 $, then substitute into $ f_1  $ we got $ 2x_1^2 = 2x_2 ^2 = 2 $. Then the two solutions are $ (x_1, x_2) = (1, 1) \text{ or } (x_1, x_2) = (-1, -1) $.

\end{solution}

\begin{subproblem}
    
    Show that one iteration of Newton's method for the solution of this system gives $ x^{(k)} = \left(x_1^{(k)}, x_2^{(k)}\right)^T $ with,
    \[
        x_1^{(1)} = x_2^{(1)} = \frac{\left|x_1^{(0)}\right|^2 + \left|x_2^{(0)}\right|^2 + 2}{2\left(x_1^{(0)} + x_2^{(0)}\right)}
    \]

\end{subproblem}

\begin{solution}
    
    First compute
    \[
        J_{f}(a, b) = \begin{bmatrix}
          2a & 2b \\
          1 & -1 \\
        \end{bmatrix}
    \]

    Then, 
    \[
        J_{f}^{-1}(a, b) = \frac{1}{-2(a + b)}
        \begin{bmatrix}
          -1 & -2b \\
          -1 & 2a \\
        \end{bmatrix}
        =
        \begin{bmatrix}
          \frac{1}{2(a + b)} & \frac{b}{a + b} \\
          \frac{1}{2(a + b)} & -\frac{a}{a + b} \\
        \end{bmatrix}
    \]

    The correction for each variable is:
    \begin{align*}
        c_1 = \frac{1}{2(a+b)}(a^2+b^2-2) + \frac{b}{a+b}(a-b) = \frac{a^2 + 2ab - b^2 - 2}{2(a+b)} \\
        c_2 = \frac{1}{2(a+b)}(a^2+b^2-2) - \frac{a}{a+b}(a-b) = \frac{-a^2 + 2ab - b^2 - 2}{2(a+b)} \\
    \end{align*}

    then update the results as:
    \begin{align*}
        x_1 = a - c_1 = \frac{a^2 + b^2 + 2}{2(a+b)} \\
        x_2 = b - c_2 = \frac{a^2 + b^2 + 2}{2(a + b)}
    \end{align*}

    This is equivalent to 
    \[
        x_1^{(1)} = x_2^{(1)} = \frac{\left|x_1^{(0)}\right|^2 + \left|x_2^{(0)}\right|^2 + 2}{2\left(x_1^{(0)} + x_2^{(0)}\right)}
    \]

\end{solution}

\begin{subproblem}
    
    Deduce that the iteration converges to $ (1,1)^T $ if $ x_1^{(0)} + x_2^{(0)} $ is positive, and if $ x_1^{(0)} + x_2^{(0)} $ is negative, the iteration converges to the other solution.

\end{subproblem}

\begin{solution}
    
    From (b) we know that each iteration $ x_1 = x_2 $, so we can let $ s = x_1 = x_2 $ and reduce the problem to 1d.

    \vspace{1em}
    The function now turns to $ f = 2s^2 - 2 = 0 $, which shows that the two solutions are $ s = \pm 1 $ and the iteration becomes:
    \[
        s_{k+1} = s_{k} - \frac{2s_{k}^2 - 2}{4s_{k}} = \frac{s_{k}^2 + 1}{2s_{k}}
    \]

    Since the numerator is strictly $ > 0 $, the sign of $ s_{k+1} $ is the same as the sign of $ s_{k} $. 

    And from the first step:
    \[
        s_1 = \frac{\left(x_1^{(0)}\right)^2 + \left(x_2^{(0)}\right)^2 + 2}{2\left(x_1^{(0)} + x_2^{(0)}\right)}
    \]

    For same reason, $ s_1 $ has the same sign as $ x_1^{(0)} + x_2^{(0)} $. Therefore, if $ x_1^{(0)} + x_2^{(0)} > 0 $, the sequence will always be positive and eventually converge to $ (1,1) $. If $ x_1^{(0)} + x_2^{(0)} < 0 $, then the sequence will always be negative and eventually converge to $ (-1, -1) $.

\end{solution}


    

\end{document}
