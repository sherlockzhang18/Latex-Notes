\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry} % reduce margin

% Basic packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{url}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage[dvipsnames]{xcolor}
\usepackage{xifthen}

% Math packages
\usepackage{amsmath, amsfonts, mathtools, amsthm, amssymb}
\usepackage{mathrsfs}
\usepackage{cancel}
\usepackage{bm}
\usepackage{systeme}
\usepackage{stmaryrd} % for \lightning

% Math shortcuts
\newcommand\N{\ensuremath{\mathbb{N}}}
\newcommand\R{\ensuremath{\mathbb{R}}}
\newcommand\Z{\ensuremath{\mathbb{Z}}}
\renewcommand\O{\ensuremath{\emptyset}}
\newcommand\Q{\ensuremath{\mathbb{Q}}}
\newcommand\C{\ensuremath{\mathbb{C}}}
\newcommand\F{\ensuremath{\mathbb{F}}}
\DeclareMathOperator{\sgn}{sgn}
\DeclareMathOperator{\Ker}{ker}
\DeclareMathOperator{\im}{Im}

% Logic symbols
\let\svlim\lim\def\lim{\svlim\limits}
\let\implies\Rightarrow
\let\impliedby\Leftarrow
\let\iff\Leftrightarrow
\let\epsilon\varepsilon
\newcommand\contra{\scalebox{1.1}{$\lightning$}}

% Useful commands
\definecolor{correct}{HTML}{009900}
\newcommand\correct[2]{\ensuremath{\:}{\color{red}{#1}}\ensuremath{\to }{\color{correct}{#2}}\ensuremath{\:}}
\newcommand\green[1]{{\color{correct}{#1}}}

% Horizontal rule
\newcommand\hr{
    \noindent\rule[0.5ex]{\linewidth}{0.5pt}
}

% Simple theorem environments (without fancy boxes for homework)
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}
\newtheorem*{example}{Example}

% Problem environment
\newcounter{problem}
\newenvironment{problem}[1][]
{
    \stepcounter{problem}
    \section*{Problem \theproblem\ifx\relax#1\relax\else: #1\fi}
}
{}

% Subproblem environment
\newcounter{subproblem}[problem]
\newenvironment{subproblem}[1][]
{
    \stepcounter{subproblem}
    \subsection*{(\alph{subproblem})\ifx\relax#1\relax\else\ #1\fi}
}
{}

% Solution environment
\newenvironment{solution}
{
    \noindent\textbf{Solution:}\\
}
{
    
}

% Headers
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Sherlock Zhang}
\fancyhead[C]{4501 - Homework 1}
\fancyhead[R]{\today}
\fancyfoot[C]{\thepage}

% Title info
\title{Math 4501 - Homework 1}
\author{Sherlock Zhang}
\date{\today}

\begin{document}

\maketitle

% =============================================================================
% HOMEWORK PROBLEMS START HERE
% =============================================================================

\begin{problem}
    Consider the simple iteration $ (x_{k})^\infty_{k = 0} $ on the function $ g: [-1, 1] \to \mathbb{R} $ given by $ g(x) = x^4 $.
\end{problem}

\begin{subproblem}
    Find all the fixed points $ \xi \in [-1, 1] $.
\end{subproblem}

\vspace{1em}
\begin{solution}
    Need to find all the $ \xi $ that $ g(\xi ) = \xi  $

    given $ g(x) = x^4 $, $ g(\xi ) = \xi ^4 = \xi  $, \hspace{2em} we get $ \xi_1 = 0 \text{ and } \xi_2 = 1  $.

\end{solution}

\vspace{1em}
\begin{subproblem}
    Is $ g $ a contraction in the neighborhood of each of the fixed points?
\end{subproblem}

\vspace{1em}
\begin{solution}
    
    If $ \xi = 0 $, assume there exists neighborhood $ (-r, r) $. If it is a contraction, then there exists $ L \in [0, 1) $ that for all $ x, y \in (-r, r) $, $ |g(x) - g(y)| \leqslant L |x-y| $, which is equivalent to $ \frac{|g(x) - g(y)|}{|x-y|} \leqslant L $. By MVT $ \frac{g(x) - g(y)}{x-y} = g'(n) $ where $ n \in [x, y] $, then $ \frac{|g(x) - g(y)|}{|x-y|} \leqslant |g'(n)| $. As $ g'(x) = |4x^3| =  4|x^3| $, it is monotonically increasing. If we choose $ r $ small enough so that $ g'(r) < 1 $, then for all $ x, y \in (-r, r) $, we have $ \frac{|g(x) - g(y)|}{|x-y|} < 1 \implies |g(x) - g(y)| \leqslant L |x-y|$ for some $ 0 \leqslant L < 1 $. So around 0 there exists contraction if the neighborhood is small.

    \vspace{1em}
    If $ \xi = 1 $, then for any arbitrary neighborhood $ N $, choose $ x, y \in N $ large enough so that $ g'(\text{min}\{|x|, |y|\}) > 1 $. Then $ \frac{|g(x) - g(y)|}{|x-y|}  =g'(n) $ where $ n \in (x,y) $. Since $ n > x $, $ g'(n) > g'(\text{min}\{|x|, |y|\}) > 1 $, which means that $ \frac{|g(x) - g(y)|}{|x-y|} > 1 $. In the form of $ |g(x) - g(y)| \leqslant L |x-y| $, $ L $ must be larger than $ 1 $. So around 1 there is no contraction.

\end{solution}

\vspace{1em}
\begin{subproblem}
    Show that the simple iteration converges for all $ x_0 \in (-1, 1) $.
\end{subproblem}

\begin{solution}
    Given $ x_{k+1} = x_{k}^4 $, $ x_{k} \geqslant 0 $ for $ k > 0 $. Therefore, the sequence is bounded below by 0. Also, since $ |x_{k}| < 1 , x_{k+1} < x_{k} $, so it is a monotonically decreasing sequence. A sequence that is bounded below and monotonically decreasing has a limit and $ x = \lim_{k \to \infty} x_{k} = \inf x_{k} = 0 $.
\end{solution}

\newpage
\begin{problem}
    The iteration defined by $ x_{k+1} = \frac{1}{2}(x_{k}^2+c) $ where $ 0 < c < 1 $ has two fixed points $ \xi_1, \xi_2 $ where $ 0 < \xi_1 < 1 < \xi_2 $. Show that $ x_{k+1} - \xi_1 = \frac{1}{2}(x_{k}+\xi_1)(x_{k}-\xi_1) $ for $ k = 1, 2, \cdots , $ and deduce that $ \lim_{k \to \infty} x_{k} = \xi_1 $ if $ 0 \leqslant x_0 < \xi_2 $. How does the iteration behave for other values of $ x_0 $?
\end{problem}

\vspace{1em}
\begin{solution}
    
    Given $ \xi_1 $ is a fixed point, by definition $ \xi_1 = \frac{1}{2}(\xi_1^2 + c) $. Therefore we can rewrite
    \begin{align*}
        x_{k+1} - \xi_1 
        &= \frac{1}{2}(x_{k}^2+c) - \frac{1}{2}(\xi_1^2 + c) \\
        &= \frac{1}{2}(x_{k}^2 - \xi_{k}^2) \\
        &= \frac{1}{2}(x_{k}+\xi_1)(x_{k}-\xi_1)
    \end{align*}

    \vspace{2em}
    For $ x_0 \in [0, \xi_2) $, define $ g(x) = \frac{1}{2}(x^2 + c) $. Then $ g'(x) = x $ is continuous. As $ x > 0 $ and $ g(\xi_2) = \xi_2 $, for all $ x \in [0, \xi_2) $, $ g(x) < \xi_2 $. So $ g: [0, \xi_2) \to [0, \xi_2) $. Also, $ x_{k+1} - \xi_1 = \frac{1}{2}(x_{k}-\xi_1)(x_{k}+\xi_1) < \frac{1}{2}(x_{k}-\xi_1)(\xi_1 + \xi_2) $. Therefore we got $ 0 < \frac{1}{2}(x_{k}-\xi_1)<\frac{1}{2}(\xi_2-\xi_1) $. By solving the equation for $ \xi_1 \text{ and } \xi_2 $, we have $ \xi_1 + \xi_2 = (1-\sqrt{1-c}) + (1 + \sqrt{1+c}) = 2 $, so $ 0 < \frac{1}{2}(x_{k} + \xi_1) < 1 $. Plugging into the equation above, we have $ x_{k+1} - \xi_1 < \frac{1}{2}(x_{k}-\xi_1)(\xi_1+\xi_2) < (x_{k}-\xi_1) $. This implies $ |x_{k+1} - \xi_1| < |x_{k}-\xi_1| $. This shows that $ x_{k} $ is getting closer to $ \xi_1 $ when $ k $ increases. So we have $ \lim_{k \to \infty} |x_{k}-\xi_1| = 0 $.

    \vspace{2em}
    For $ x_0 < 0 $, it yields the same result as starting with $ -x_0 $, so only consider $ |x_0| $.

    \vspace{1em}
    If $ |x_0| \in [0, \xi_2) $, we have shown above it converges to $ \xi_1 $.

    \vspace{1em}
    If $ |x_0| \geqslant \xi_2 $, then $ x_{k+1} - \xi_2 = \frac{1}{2}(x_{k}+\xi_2)(x_{k}-\xi_2) $. Since both $ x_{k} \text{ and } \xi_2 >1 $, their sum $ >2 $. So $ x_{k+1} - \xi_2 > (x_{k}-\xi_2) $. $ \lim_{k \to \infty}x_{k}-\xi_2 = \infty $.

\end{solution}

\newpage
\begin{problem}
    Lef $ f: [a,b] \to \mathbb{R} $ be a continuous function with $ f(a) < 0 < f(b) $. Recall the two sequences $ (a_{i})_{k=0}^\infty $ and $ (b_{i})_{k=0}^\infty $ generated by the bisection method. Show that both sequences are Cauchy, and that they converge to the same limit.
\end{problem}

\vspace{1em}
\begin{solution}

    By the construction of $ (a_{i})_{k=0}^\infty $ and $ (b_{i})_{k=0}^\infty $, $ a_{i} < 0 $ for all $ i $ and is monotonically increasing, $ b_{i} < 0 $ for all $ i $ and is monotonically decreasing. Therefore, for any $ n < m $, we have
    \begin{align*}
        |a_{n} - a_{m}| < |a_{n} - 0| < |a_{n} - b_{n}| \\
        |b_{n} - b_{m}| < |b_{n} - 0| < |a_{n} - b_{n}|
    \end{align*} 

    As $ |a_{n} - b_{n}| \leqslant 2^{-n} \cdot |a_0-b_0| $ from the proof we did in class, we can choose $ N $ large enough that $ 2^{-N} \cdot |a_0-b_0| < \epsilon $. Then, let $ N \leqslant n < m $, we have
    \[
        |a_{n} - a_{m}| < |a_{n} - 0| < |a_{n} - b_{n}| \leqslant |a_{N} - b_{N}| \leqslant 2^{-N} \cdot |a_0-b_0| < \epsilon
    \]
    \[
        |b_{n} - b_{m}| < |b_{n} - 0| < |a_{n} - b_{n}| \leqslant |a_{N} - b_{N}| \leqslant 2^{-N} \cdot |a_0-b_0| < \epsilon
    \]

    Therefore, both sequences are Cauchy. Moreover, 
    \begin{align*}
        &0 < |a_{n} - 0| = -a_{n} < 2^{-n} \cdot |a_0-b_0| \implies a_{n} > -2^{-n} \cdot |a_0-b_0| \\
        &0 < |b_{n} - 0| = b_{n} < 2^{-n} \cdot |a_0-b_0|
    \end{align*}
    as $ n $ increases, both sequences becomes arbitrarily close to 0. 
    \begin{align*}
        \lim_{n \to \infty} a_{n} = \lim_{n \to \infty} b_{n} = 0
    \end{align*}

\end{solution}

\newpage
\begin{problem}
    
    Let $ g : \mathbb{R} \to \mathbb{R} $ be a contraction, with Lipschitz constant $ L = \frac{1}{2} $. Consider the simple iteration $ x_{k+1} = g(x_{k}) $.

\end{problem}

\begin{subproblem}
    Suppose you want the error $ |x_{k} - \xi| $ to be below the error tolerance $ \epsilon_0 = 10^{-5} $. How many iteration are needed for $ x_{k} $ to be within $ \epsilon_0 $ of its point of convergence?
\end{subproblem}

\vspace{1em}
\begin{solution}
    As $ g $ is a contraction, it converges. By property of cauchy, the series $ (x_{k})_{k=0}^\infty $ is Cauchy. Therefore, given $ \epsilon_0 = 10^{-5} $, we only need to choose $ N $ large enough so that for any $ N \leqslant n < m $, which implies that $ |g(x_n) - g(x_m)| < \epsilon_0 $. Given $ \xi = \lim_{m \to \infty}g(x_{m}) $, $ |g(x_{n}) - \xi| = \lim_{m \to \infty}|g(x_{n})-g(x_{m})| \leqslant  \epsilon_0 $. From the proof in class we get
    \[
        N > \frac{1}{\log L}\log\left(\frac{\epsilon_0(1-L)}{|x_1-x_0|}\right) = \frac{1}{\log \frac{1}{2}}\log\left(\frac{10^{-5}\cdot \frac{1}{2}}{|x_1-x_0|}\right)
    \]
\end{solution}

\vspace{2em}
\begin{subproblem}
    Now suppose that you make some error in your simple iteration. That is, at each iteration we have $ x_{k+1} = g(x_{k}) + e_{k} $ instead of the simple iteration. You do not know what the error is, but suppose you know that the error decays at the rate $ |e_{k}| < \alpha^k $, where $ \alpha = \frac{1}{3} $.

    \noindent Does the simple iteration still converge to the unique fixed point? Explain why or why not.
\end{subproblem}

\vspace{1em}
\begin{solution}
    let $ d_{k} = |x_{k} - \epsilon| $. Using the inequality in proof theorem 1.5, we have,
    \[
        d_{k+1} \leqslant  |g(x_{k}) - g(\xi)| + |e_{k}| \leqslant Ld_{k} + \alpha^k
    \]
    Unpack the recursion we have
    \[
        d_{k} \leqslant L^kd_0 + \sum_{j = 0}^{k-1} L^{k-1-j}\alpha^j
    \]

    Using sum of geometric series, we have
    \[
        \sum_{j = 0}^{k-1} L^{k-1-j}\alpha^j = L^kd_0 + \frac{L^k-\alpha^k}{L-\alpha} \hspace{1em}(L \neq \alpha)
    \]
    and if $ L = a $, then $ d_{k} \leqslant L^k d_0 + kL^{k-1} $

    Since both $ L $ and $ \alpha < 0 $, both right hand side has $ \lim \text{RHS} = 0 $ when $ k \to \infty $.

    Therefore, the iteration still converges.
\end{solution}

\newpage
\begin{problem}
    
    Define the function $ g: [-1, 1] \to \mathbb{R} $ by
    \[
        g(x) = \begin{cases}
            0  & \text{if } |x| = 0, \\
            -x \sin^2(\frac{1}{x})  & \text{if } 0 < |x| \leqslant 1. \\
        \end{cases}
    \]

\end{problem}

\begin{subproblem}
    Show that $ g $ is continuous, and that $ 0 $ is the only fixed point of $ g $ in the interval $ [-1, 1] $.    
\end{subproblem}

\vspace{1em}
\begin{solution}
    For $ x \neq 0 $, $ g $ is a composition of continuous functions on $ [-1, 0) \cup (0, 1] $. By continuity property it is continuous.

    For $ x = 0 $, for all $ a $ that 
    \[
        |g(a) - g(0)| = |-a\sin^2(\frac{1}{a})| \leqslant |a| \cdot 1 = |a|
    \] as $ \sin(x) \leqslant 1 $

    Therefore, for any $ \epsilon > 0 $, choose $ \delta = \epsilon $, then $ 0 < |a| < \delta \implies |g(a) - g(0)| \leqslant |a| < \epsilon $. Therefore $ g $ is continuous at $ 0 $.

    \vspace{2em}
    For the fixed points, we need to solve $ g(x) = x $.

    For $ x = 0 $, $ g(0) = 0 $. So $ 0 $ is a fixed point.

    \vspace{1em}
    For $ x \neq 0 $, $ g(x) = -x \sin^2(\frac{1}{x}) = x $. Then $ \sin^2(\frac{1}{x}) = -1 $. No solution.

    \vspace{1em}
    Therefore, the only fixed point is 0


\end{solution}

\begin{subproblem}
    
    Does the simple iteration on $ g $ converge if $ x_0 = \frac{j}{\pi} $ for some $ j \in {1,2} $? How about if $ x_0 = \frac{2}{(2j+1)\pi} $ for $ j \in \mathbb{Z} $?

\end{subproblem}

\vspace{1em}
\begin{solution}
    For $ x_0 = \frac{1}{\pi} $, $ x_1 = g(\frac{1}{\pi}) = -\frac{1}{\pi} \sin^2(\pi) = 0 $. So it converges.

    \vspace{2em}
    For $ x_0 = \frac{2}{\pi} $, $ x_1 = g(\frac{2}{\pi}) = -\frac{2}{\pi} \sin^2(\frac{\pi}{2}) = -\frac{2}{\pi} $. Then $ x_2 = \frac{2}{\pi} $. The series is bouncing periodically but not converging to a fixed point.

    \vspace{2em}
    For $ x_0 = \frac{2}{(2j+1)\pi} $, we have
    \[
        g(x_0) = -x_0 \sin^2(\frac{1}{x_0}) = -x_0 \sin^2\left(\frac{(2j+1)\pi}{2}\right) = -x_0 \cdot 1 = -x_0
    \]

    Therefore, $ x_1 = -x_0 $, $ x_2 = -x_1, \cdots  $. The series is still periodically bouncing. So it doesn't converge.
\end{solution}

\end{document}
