\lesson{13}{Wed Sep 24 2025 15:00}{}

\underbar{\textbf{Matrix Factorization}}: LU decomposition, $ \alpha\mathbb{R} $factorization, SVD

$ 
    \begin{cases}
        &x_1 + x_2 + x_3 = 6 \\
        & 2x_1+4x_2+2x_3 = 16 \\
        & -x_1+5x_2-4x_3 = -3 \\
    \end{cases} 
$ Use Gaussian dlimination

which is equivalent to 
\[
    \begin{bmatrix}
      1 & 1 & 1 \\
      2 & 4 & 2 \\
      -1 & 5 & -4 \\
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\
      x_2 \\
      x_3 \\
    \end{bmatrix}
    =
    \begin{bmatrix}
      6 \\
      16 \\
      -3 \\
    \end{bmatrix}
\]

Reduce it from
\[
    \begin{bmatrix}
      1 & 1 & 1 \\
      2 & 4 & 2 \\
      -1 & 5 & -4 \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 2 & 0 \\
      -1 & 5 & 4 \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 2 & 0 \\
      0 & 6 & 3 \\
    \end{bmatrix}
    \rightarrow
    \begin{bmatrix}
      1 & 1 & 1 \\
      0 & 2 & 0 \\
      0 & 0 & -3 \\
    \end{bmatrix}
\]

Every row operation can be viewed as matrix multiplication of $ I + \mu E^{(r,s)} $ where $ E^{(r,s)}  $ has 1 on row r column s and 0 everywhere else.

For example, first operation above is equivalent to 
\[
    \begin{bmatrix}
      1 & 0 & 0 \\
      -2 & 1 & 0 \\
      0 & 0 & 1 \\
    \end{bmatrix}
    \times
    \begin{bmatrix}
      1 & 1 & 1 \\
      2 & 4 & 2 \\
      -1 & 5 & -4 \\
    \end{bmatrix}
\]



\begin{definition} Lower Triangular matrix ($ L $) \leavevmode
    
    $ L \in \mathbb{R}^{n \times n} $ is lower triangular if (ij)-th entry $ L_{ij} = 0 $ where $ 1 \leqslant i < j \leqslant n $

    \vspace{1em}
    Unit-lower-triangular if $ L $ is lower-triangular and has $ L_{ii} = 1, i = 1, \cdots , n $.

\end{definition}


\vspace{2em}
\begin{theorem}
    
    For any integer $ n \geqslant 2 $, 
    \begin{enumerate}
        \item product of 2 ($ n \times n $) triangular matrices is triangular.
        \item product of 2 ($ n \times n $) triangular matrices is unit-lower triangular.
        \item lower-triangular matrix non-singular iff diagonal entries are non-zero.
        \item $ n \times n $ lower-triangular matrix has a lower-triangular inverse. (Assuming it's non-singular)
        \item $ n \times n $ unit-lower-triangular matrix has a unit-lower-triangular inverse. (Assuming it's non-singular)
    \end{enumerate}

\end{theorem}

\begin{proof}
    prove 4 by induction.

    base step: $ n = 2 $. $ \begin{bmatrix}
      a & b \\
      c & d \\
    \end{bmatrix}
    =
    \frac{1}{ad-bc}
    \begin{bmatrix}
      d & -b \\
      -c & a \\
    \end{bmatrix} $

    \vspace{1em}
    Induction-step: Assume lower-triangular matrix of size $ \leqslant k $ have lower-triangular inverses, then show ower-triangular matrix of size $ k + 1 $ have lower-triangular inverses.

    $ L \in \mathbb{R}^{k \times k} $, lower-triangular. 
    Extract the last row and column out, then
    \[
        \overrightarrow{L} = \begin{bmatrix}
          \overrightarrow{L_1} & \overrightarrow{0} \\
          \overrightarrow{r}^T & \alpha \\
        \end{bmatrix}
    \]

    Now $ L_1 \in \mathbb{R}^{(k-1)\times(k-1)} $

    $ \overrightarrow{L_1} = \begin{bmatrix}
      \overrightarrow{X} & \overrightarrow{y} \\
      \overrightarrow{z}^T & \beta \\
    \end{bmatrix} $

    \vspace{1em}
    \[
        I_{k} = L \cdot L^{-1} =
        \begin{bmatrix}
          L_1X & L_1y \\
          r^T X + \alpha z^T & r^T y + \alpha + \beta \\
        \end{bmatrix}
    \]

    \begin{itemize}
        \item $ L_1X = I_{k-1} $, by induction hypothesis, $ X = L_1^-1 $ is lower-triangular.
        \item $ L_1y = 0 \implies y = 0 $
    \end{itemize}

    These two facts above shows that $ \overrightarrow{L}^{-1} $ is lower-triangular.

\end{proof}

\begin{definition}
    
    \[
        (I + \mu_{(N)} E_{(N)}^{(r,s)})
        (I + \mu_{(N)} E_{(N)}^{(r,s)})
        (I + \mu_{(N)} E_{(N)}^{(r,s)})
        A = U
    \], where $ U $ is a uppertriangular.

    and $ (I + \mu_{(N)} E_{(N)}^{(r,s)}) $ is unit lower triangular.

    From the theorem above, $ (I + \mu_{(N)} E_{(N)}^{(r,s)})
        (I + \mu_{(N)} E_{(N)}^{(r,s)})
        (I + \mu_{(N)} E_{(N)}^{(r,s)}) $ is also lower-triangular.

    Gaussian Elimination: $ L_(w) \cdots L_(3)L_(2)L_(1) A = U $ where $ U $ is upper-triangular.

    LU Decomposition:
    \[
        \overrightarrow{A} = \overrightarrow{L}\overrightarrow{U}, \overrightarrow{L} = L_(1)^{-1}L_(2)^{-1} \cdots L_(N)^{-1}
    \]

\end{definition}

For LU decomposition, $ a_{ij} = \sum_{k = 1}^{n} l_{ik}u_{kj} (1\leqslant i,j \leqslant n) $

but a lot of terms are zero due to upper and lower triangular matrix, so 

$ \begin{cases}
    a_{ij} = \sum_{k=1}^{i} l_{ik}u_{kj} \hspace{2em} i<j \\
    a_{ij} = \sum_{k=1}^{j} l_{ik}u_{kj} \hspace{2em} i\geqslant j \\
\end{cases} $

\vspace{1em}
$ u_{ij} = a_{ij} - \sum_{k = 1}^{i-1} l_{ik}u_{kj} $

$ l_{ij} = \frac{1}{u_{jj}}\left( a_{ij} - \sum_{k = 1}^{i-1} l_{ik}u_{kj} \right) $

There is potential of dividing by zero for $ u_{jj} $.

During Elimination, "pivot" the rows.