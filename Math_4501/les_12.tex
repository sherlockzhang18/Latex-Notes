\lesson{12}{Mon Sep 22 2025 15:00}{}

\begin{itemize}
    \item $ \overrightarrow{A}^T \overrightarrow{A}: $ symmetric matrix $ \implies $ 1. eigenvalues are real. 2. orthonormal set of eigenvectors $ (\overrightarrow{v_{j}})_{j = 1} ^n $: $ \overrightarrow{v_i}\cdot \overrightarrow{v_{j}} = 1 $ if $ i = j $ and $ =0 $ if $ i \neq j $.
    
    \item Eigenvalues of $ \overrightarrow{A}^T \overrightarrow{A}, \lambda_{j}\geqslant 0 $
    
    If $ \lambda_j < 0 $ is an eigenvalue, $ \overrightarrow{v_{j}} $ is the corresponding eigenvector, then 
    \[
        \overrightarrow{A}^T \overrightarrow{A} = \lambda_{j}\overrightarrow{v_{j}}
    \]

    
    $ \left\lVert \overrightarrow{A} \overrightarrow{v_{j}} \right\rVert _2^2 = \overrightarrow{v_{j}}^T \overrightarrow{A} ^T \overrightarrow{A} \overrightarrow{v_{j}} = \lambda_{j} \overrightarrow{v_{j}} ^T \overrightarrow{v_{j}} = \lambda_{j} \left\lVert \overrightarrow{v_{j}} \right\rVert _2^2 $, which reaches contradiction since the lhs $ \geqslant 0 $ and rhs is a product of $ > 0 $ and $ <0 $.
\end{itemize}

\begin{theorem}
    $ \left\lVert \overrightarrow{A} \right\rVert _2 = \max_{i} \lambda_i ^{\frac{1}{2}} $, $ \lambda_{i} \text{eigenvalue of } \overrightarrow{A}^T \overrightarrow{A}, \overrightarrow{A} \in \mathbb{R}^{m \times n} $
\end{theorem}

\begin{proof} \leavevmode
    
    Let $ (\overrightarrow{v_{j}})_{j = 1}^n $ be orthonormal set eigenvectors of $ \overrightarrow{A} ^T \overrightarrow{A} $. Given $ \overrightarrow{x} \in \mathbb{R}^n $, we can write $ \overrightarrow{x} = c_1 \overrightarrow{v_1} + c_2 \overrightarrow{v_2} + \cdots + cn \overrightarrow{v_{n}} $

    \vspace{1em}
    Recall the ratio $ \frac{\left\lVert \overrightarrow{A}\overrightarrow{x} \right\rVert _2^2}{\left\lVert \overrightarrow{x} \right\rVert _2^2} $

    \begin{align*}
        \left\lVert \overrightarrow{A}\overrightarrow{x} \right\rVert 
        &= \overrightarrow{x} ^T \overrightarrow{A} ^T \overrightarrow{A} \overrightarrow{x} = (c_1 \overrightarrow{v_1} + \cdots + c_{n} \overrightarrow{v_{n}}) ^T \overrightarrow{A} ^T \overrightarrow{A} + (c_1 \overrightarrow{v_1} + \cdots + c_{n} \overrightarrow{v_{n}}) \\
        &= (c_1 \overrightarrow{v_1} + \cdots + c_{n} \overrightarrow{v_{n}}) (\overrightarrow{c_1A} ^T \overrightarrow{A} \overrightarrow{v_1} + \cdots + cn \overrightarrow{A} ^t \overrightarrow{A} \overrightarrow{v_{n}}) \\
        &=(c_1 \overrightarrow{v_1} + \cdots + c_{n} \overrightarrow{v_{n}}) ^T (c_1\lambda_1 \overrightarrow{v_1} + \cdots + c_{n}\lambda_{n}\overrightarrow{v_{n}}) \\
        &= \sum_{j = 1}^{n} \sum_{i = 1}^{n} (c_{i} \overrightarrow{v_{i}}) ^T (c_{j}\lambda_{j} \overrightarrow{v_{j}}) \\
        &=  \sum_{j = 1}^{n} \sum_{i = 1}^{n} \lambda_{j} c_{i} c_{j} \overrightarrow{v_{i}} ^T \overrightarrow{v_{j}}\\
        &=  \sum_{i = 1}^{n} \lambda_{i}c_{i}^2 \left\lVert  \overrightarrow{v_{i}} \right\rVert _2^2 = \sum_{i = 1}^{n} \lambda_{i} c_{i}^2 \\
    \end{align*}

    \vspace{2em}
    $ \left\lVert \overrightarrow{x} \right\rVert _2^2 = \sum_{i = 1}^{n} c_1^2 $ (Why?)

    $ \left\lVert \overrightarrow{x} \right\rVert _2^2  = \overrightarrow{x} ^T \overrightarrow{x} = (c_1 \overrightarrow{v_1} + \cdots + c_{n} \overrightarrow{v_{n}}) ^T (c_1 \overrightarrow{v_1} + \cdots + c_{n} \overrightarrow{v_{n}}) = \sum_{i=1}^{n} c_1^2 \left\lVert \overrightarrow{v_{i}} \right\rVert _2^2 $ 

    \vspace{2em}
    $ \sum_{i=1}^{n} \lambda_{i} c_1^2 \leqslant \left\lVert \overrightarrow{a} \right\rVert _{\infty} \left\lVert \overrightarrow{b} \right\rVert _1 = (\max_{i} \lambda_{i})\sum_{i = 1}^{n} |c_{i}|^2 =  (\max_{i} \lambda_{i}) \left\lVert \overrightarrow{x_{i}} \right\rVert _2^2 $.

    So the ratio is $ \frac{\left\lVert \overrightarrow{A}\overrightarrow{x} \right\rVert _2^2}{\left\lVert \overrightarrow{x} \right\rVert _2^2} \leqslant 
    \frac{(max_{i} \left\lVert \overrightarrow{x} \right\rVert) _2^2}{\left\lVert \overrightarrow{x} \right\rVert _2^2} \implies \left\lVert \overrightarrow{A} \right\rVert _2^2 \leqslant (\max_{i}\lambda_{i} ^{\frac{1}{2}})$

    \vspace{2em}
    Pick $ \overrightarrow{x_{k}} = \overrightarrow{v_{i}}_k, i_{k} = \text{argmax} \lambda_{i} $
    \[
        \frac{\left\lVert \overrightarrow{A}\overrightarrow{x} \right\rVert _2^2 }{\left\lVert \overrightarrow{x} \right\rVert _2^2}
        = \frac{\overrightarrow{x_{k}} \overrightarrow{A} ^T \overrightarrow{A} \overrightarrow{x_{k}}}{\overrightarrow{x_{k}} ^T \overrightarrow{x}}
        = \frac{\overrightarrow{v_{ik}} \overrightarrow{A} ^T \overrightarrow{A} \overrightarrow{v_{ik}}}{1}
        = \overrightarrow{v_{ik}} ^T (\lambda_{ik} \overrightarrow{v_{ik}})
        = \lambda_{ik} \left\lVert \overrightarrow{v_{ik}} \right\rVert _2^2 = \lambda_{ik}
    \]

    This shows $ \left\lVert \overrightarrow{A} \right\rVert _2^2 \geqslant \lambda_{i} ^{\frac{1}{2}} $

\end{proof}

\vspace{2em}
\begin{eg} \leavevmode

    \begin{itemize}
        \item $ \overrightarrow{A} = \overrightarrow{I} $ (Identity matrix) \hspace{2em} $ \left\lVert \overrightarrow{A} \right\rVert _2 = 1 $
        
        \item $ \overrightarrow{A} = \overrightarrow{D} $ Diagonal square matrix \hspace{2em} $ \left\lVert \overrightarrow{A} \right\rVert _2 = \max_{i}|d_{i}| $
        
        \[
            \overrightarrow{D} ^T \overrightarrow{D} 
        \]

        \item $ A = Q $ orthogonal matrix, $ Q ^T Q = I $\hspace{2em} $ \left\lVert A \right\rVert _2 = 1 $
        
        \item $ A = DQ $ \hspace{2em} $ \left\lVert A \right\rVert _2 = \max_{i}|d_{i}| $
        
        $ A A^T = (DQ)(DQ)^T = D Q Q ^T D = D^2 $

        given $ u,v $ orthogonal, $ D $ diagonal, then $ A = UDV^T $, $ \left\lVert A \right\rVert _2 = \max_{i}|d_{i}|$
    \end{itemize}

\end{eg}